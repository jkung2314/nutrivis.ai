System and Unit Test Report
Nutrivis.ai
12.02.2018

Brief Summary of our Automated Testing:

  For our testing, we utilized Travis CI for our build system. This automated
  build system will start with a clean build and install of our app after each
  push to any of our branches, with the master branch being the default branch. It
  should be noted that the build task also consists of a run of the android lint
  tool, which checks our code and XML files, and identifies any potential
  structural issues in our code, like deprecated packages or functional issues in
  the XML design. It helps to identify bugs and optimize for things like security
  and performance. It especially useful as it helped define our definition of done.

  After the clean build and install finishes without errors, our automated build
  system will proceed to run our test suite of unit and integration tests. Our
  test suite consists of unit and integration tests of our core API classes,
  which is the implementation of the Nutritionix API and the Google Vision API.
  We verified the integrity of our Google Vision API, and provided a test image to
  make sure that we were getting the correct response from the server. We did the
  same for the other by providing a test food to the Nutritionix API in the unit +
  integration tests to verify the correct response.

System Test Scenarios

User Stories:
  A. User story 1 from sprint 1: As a user, I want to be able to have a login
      authentication so that I know my data is secure.

  B. User story 1 from sprint 2: As a user, I want to be able to take a picture
      in the app.

  C. User story 2 from sprint 2: As a user, I want to able to have the app know
      what the object is.

  D. User story 1 from sprint 3: As a user, I want to be able to retrieve nutrition
      information and have it properly displayed in the UI so that I know the nutrition
      information for the food I am looking at.

  E. User story 1 from sprint 4: As a user, I want to see a polished app UI so that
      I can easily navigate and not have any trouble with the app.

Scenario:
  1. A
    - Start Nutrivis.ai app, select 'Sign in with email'
    - Type in for Email: <test@gmail.com>
    - Press 'Next' button
    - Type in for First & last name: <test tester>, and Password: <pass123>
    - Press 'Save' button
    - User should be taken to the main screen

  2. B, C, D, E
    - Click on the green button on the bottom right of the main screen
    - Camera view should open
    - Take a picture of a Banana by clicking on camera button in the middle of
        the screen
    - Click the checkmark button to confirm the photo
    - User should be redirected back to the main view with a row that displays
        the picture with the title of 'Banana' along with the date the food was
        scanned
    - Click on the row that corresponds to 'Banana'
    - The user should be redirected to a new view that shows the nutrition information,
        which includes things like calorie count and fat content
    - Type in <2.0> for the 'Servings' section
    - User should see Calorie Count and Fat Content doubled
    - Click on the back button at the bottom of the screen
    - User should be returned to the main view
    - Repeat this process with any other food and the user should be presented
        with a new row with that corresponding food nutrition info
    - Click on the logout button on the top right corner
    - User should be logged out and returned to the authentication view

Unit Tests
  Jonathan: I tested both of our core APIs, which involved the classes GVision.java,
    NutritionixParser.java, and NutritionixTask.java . There were manual tests as
    well as automated unit and integration tests, which were integrated into our
    automated build system. The equivalence class for GVision.java was any photo as
    a provided file path. The test cases sent a test photo into the Google Vision API
    and verified the correct response was returned back. For the NutritionixTask.java
    test cases, a test food, "taco", was sent, and the response was verified, while
    also making sure the NutritionixParser class was functional by verifying the object
    that was created. The equivilence class for Nutritionix task was any food. For
    reference, these tests can be found at: 
    nutrivis.ai/Nutrivis/app/src/test/java/com/example/srini/nutrivisai
    Along with this, I helped perform testing on the usability of the overall app.

    Aravind: I tested the Camera integration, which involved testing both the UI and backend. 
    	This was done in CameraActivitiy.java. The equivalency class for the CameraActivity 
	was just making sure that it was being called during the correct time.The test 
	cases verified that it was being called correctly, and that a photo can be taken
	and stored. I also worked on testing the validation of foods that were identified by 
	Google Vision API. The equivalency class for this was to make sure I received 
	a map of predictions from the Google Vision API. The test case for this included 
	making sure that I was able to validate that a prediction was inside the text file.
	I also integrated the entire app by connecting the separate components,
	so I tested a lot to make sure the flow inside the app was correct. The 
	equivalency classes for this was just standard input that was being passed 
	around from one class/activity to another. The test cases for this included 
	just making sure the app was flowing well from end to end. 

  Srini: I tested the StringToFoodParser, NutritionixParser, AuthUIActicity,
  CustomAdapter, DetailActivity, DownloadImageTask, Food, and MainActivity. For the 
  parsers I manually inputed the input that resembled the required input and ensured that
  the output was correct. For testing the UI activity I passed in the proper input from   
  the Db and ensured it was display in the form we had discussed. DownloadImageTask is 
  linked with DetailActivity and Mainactivity so it was tested when the said UIActivities 
  were tested. The food class was tested manually by passing the proper arguments and 
  ensuring the proper Food object was created and the information corresponded.

